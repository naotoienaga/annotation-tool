{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"semi-automatic_gesture_annotation_tool.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNN8jdhQIrYjLO77kKw7ZSI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"h1HhnZH1ugDo"},"source":["# The semi-automatic gesture annotation tool in ELAN format."]},{"cell_type":"markdown","metadata":{"id":"vRlIWY9rMIam"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"LHHouRIGMHuD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608869247233,"user_tz":-540,"elapsed":31103,"user":{"displayName":"Naoto Ienaga","photoUrl":"","userId":"16423102146717908802"}},"outputId":"ad56a99c-f54d-4e34-ff03-8ae115e441aa"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jjb6whZ5vavG"},"source":["## Parameters that should be set manually."]},{"cell_type":"code","metadata":{"id":"Te9OZz7CufOM","executionInfo":{"status":"ok","timestamp":1608873211070,"user_tz":-540,"elapsed":561,"user":{"displayName":"Naoto Ienaga","photoUrl":"","userId":"16423102146717908802"}}},"source":["# < MANDATORY >\r\n","# Path to directory of json files OpenPose generated\r\n","path_openpose_json_zip = \"/content/drive/My Drive/Colab Notebooks/json.zip\"\r\n","# Path to Elan file with some annotations of the input video (ELAN file with predicted annotations will be output to the same location)\r\n","path_elan = \"/content/drive/My Drive/Colab Notebooks/me_anno_tool.eaf\"\r\n","# FPS of the input ivdeo\r\n","fps = 25.0\r\n","# Seconds of annotations to add at one cycle\r\n","query_sec = 5\r\n","# Seconds of annotations to smooth (smooth_sec <= 0 disable smoothing)\r\n","smooth_sec = 0.2\r\n","\r\n","\r\n","# < Advanced >\r\n","# Keypoints with confidence values below this value will be interpolated\r\n","th_opconf = 0.5\r\n","# Window size (sec.) of the feature array\r\n","time_window = 0.5\r\n","# LightGBM parameters\r\n","param = (5, 31, 30, 7)\r\n","# Value ​​for splitting annotated data into train data and validation data (X means, train : validation = X-1 : 1)\r\n","n_splits = 3\r\n","# The number of epochs\r\n","epochs = 99999\r\n","# Early stopping round\r\n","early_stopping_rounds = 50"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HVj6mzSRHbNp"},"source":["## If you just would like to try this tool, just run this cell (please do not change anything)."]},{"cell_type":"code","metadata":{"id":"vnK3wg49LL3d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608873228760,"user_tz":-540,"elapsed":9221,"user":{"displayName":"Naoto Ienaga","photoUrl":"","userId":"16423102146717908802"}},"outputId":"816d7420-494b-4cce-f370-ff860834db48"},"source":["import json, copy, time, zipfile, re, ast\r\n","import numpy as np\r\n","import pandas as pd\r\n","import lightgbm as lgbm\r\n","from sklearn.model_selection import StratifiedKFold\r\n","\r\n","my_round_int = lambda x: int((x * 2 + 1) // 2)\r\n","\r\n","\"\"\"\r\n","Make input feature for LGBM based on the keypoints detected by OpenPose\r\n","\"\"\"\r\n","def make_input_feature(zipfile_, jsons_path, time_window, th_opconf):\r\n","    keypoints = np.full((len(jsons_path), 48+1, 2), np.nan, dtype=np.float64)\r\n","    body_idxes = [2, 5, 3, 6, 4, 7]\r\n","\r\n","    for frm_idx, json_path in enumerate(jsons_path):\r\n","        # read json\r\n","        with zipfile_.open(json_path) as tmp:\r\n","            json_data = ast.literal_eval(tmp.read().decode(\"UTF-8\"))\r\n","        if not json_data[\"people\"]: continue\r\n","\r\n","        # read keypoints\r\n","        pose_kpts  = json_data[\"people\"][0][\"pose_keypoints_2d\"]\r\n","        rhand_kpts = json_data[\"people\"][0][\"hand_right_keypoints_2d\"]\r\n","        lhand_kpts = json_data[\"people\"][0][\"hand_left_keypoints_2d\"]\r\n","\r\n","        # neck\r\n","        if pose_kpts[1*3+2] >= th_opconf:\r\n","            keypoints[frm_idx, -1, :] = pose_kpts[1*3], pose_kpts[1*3+1]\r\n","\r\n","        # shoulders, elbows, wrists\r\n","        kpt_idx = 0\r\n","        for body_idx in body_idxes:\r\n","            if pose_kpts[body_idx*3+2] >= th_opconf:\r\n","                keypoints[frm_idx, kpt_idx, :] = pose_kpts[body_idx*3], pose_kpts[body_idx*3+1]\r\n","            kpt_idx += 1\r\n","\r\n","        # hands\r\n","        for hand_idx in range(21):\r\n","            if rhand_kpts[hand_idx*3+2] >= th_opconf:\r\n","                keypoints[frm_idx, kpt_idx, :] = rhand_kpts[hand_idx*3], rhand_kpts[hand_idx*3+1]\r\n","            kpt_idx += 1\r\n","        for hand_idx in range(21):\r\n","            if lhand_kpts[hand_idx*3+2] >= th_opconf:\r\n","                keypoints[frm_idx, kpt_idx, :] = lhand_kpts[hand_idx*3], lhand_kpts[hand_idx*3+1]\r\n","            kpt_idx += 1\r\n","    \r\n","    # interpolation (keypoints with all frames nan are not interpolated)\r\n","    for kpt_idx in range(keypoints.shape[1]):\r\n","        if np.any(np.isnan(keypoints[:, kpt_idx, :])) and not np.all(np.isnan(keypoints[:, kpt_idx, :])):\r\n","            df = pd.DataFrame(keypoints[:, kpt_idx, :])\r\n","            df.interpolate('linear', limit_direction='both', inplace=True)\r\n","            keypoints[:, kpt_idx, :] = df.values\r\n","\r\n","    # check if the keypoint normalization is possible\r\n","    is_normalization = True\r\n","    for frm_idx in range(keypoints.shape[0]):\r\n","        check = not np.any(np.isnan(keypoints[frm_idx, 0, :])) and not np.any(np.isnan(keypoints[frm_idx, 1, :])) and not np.any(np.isnan(keypoints[frm_idx, -1, :]))\r\n","        assert check, \"Since OpenPose did not detect enough keypoints, the keypoint normalization will not be performed. If the normalization is required, consider lowering \\\"th_opconf\\\".\"\r\n","        is_normalization = False\r\n","        break\r\n","\r\n","    # normalization\r\n","    if is_normalization:\r\n","        for frm_idx in range(keypoints.shape[0]):\r\n","            shoul_len = np.linalg.norm(keypoints[frm_idx, 0, :] - keypoints[frm_idx, 1, :])\r\n","            keypoints[frm_idx] = (keypoints[frm_idx] - keypoints[frm_idx, -1, :]) / shoul_len\r\n","    \r\n","    # delete neck\r\n","    keypoints = np.delete(keypoints, -1, axis=1)\r\n","\r\n","    # keypoints -> feature\r\n","    x_data = []\r\n","    x_data_ap = x_data.append\r\n","    for idx in range(keypoints.shape[0]-time_window+1):\r\n","        pos_avg = np.average(keypoints[idx:idx+time_window], axis=0) # 48 x 2\r\n","\r\n","        # averaged distance\r\n","        dsts = np.full((time_window, keypoints.shape[1]), np.nan, dtype=np.float64)\r\n","        for dsts_idx, idx2 in enumerate(range(idx, idx+time_window)):\r\n","            dsts[dsts_idx] = np.linalg.norm(keypoints[idx2]-keypoints[int(idx+(time_window-1)/2)], axis=1)\r\n","        dsts = np.delete(dsts, int((time_window-1)/2), axis=0)\r\n","        dst_avg = np.average(dsts, axis=0) # 48\r\n","\r\n","        x_data_ap(np.hstack((np.ravel(pos_avg), dst_avg)))\r\n","\r\n","    return np.array(x_data)\r\n","\r\n","\"\"\"\r\n","Read annotation from .eaf file\r\n","\"\"\"\r\n","def read_eaf(path_elan, num_frm, fps):\r\n","    # open elan file\r\n","    eaf_file = open(path_elan)\r\n","    lines = eaf_file.readlines()\r\n","    eaf_file.close()\r\n","\r\n","    # 0:rest, 1:gesture, 2:no annotation\r\n","    annotation = np.ones(num_frm, dtype=np.uint8) * 2\r\n","    times = {}\r\n","\r\n","    for idx, line in enumerate(lines):\r\n","        # time slot\r\n","        if \"TIME_SLOT_ID=\\\"ts\" in line:\r\n","            split_ = line.split('\"')\r\n","            times[split_[1]] = float(split_[3])\r\n","\r\n","        # time slot -> frame\r\n","        elif \"<ANNOTATION_VALUE>gesture\" in line or \"<ANNOTATION_VALUE>rest\" in line:\r\n","            split_ = prev_line.split('\"')\r\n","            start = my_round_int(fps * times[split_[3]] / 1000.)\r\n","            end = my_round_int(fps * times[split_[5]] / 1000.)\r\n","            tmp = 1 if \"gesture\" in line else 0\r\n","            annotation[start:end] = tmp\r\n","\r\n","        prev_line = line\r\n","    \r\n","    return annotation\r\n","\r\n","\"\"\"\r\n","Do active learning\r\n","\"\"\"\r\n","def active_learning(feature, annotation, param, n_splits, epochs, early_stopping_rounds, query_frm):\r\n","    # train, validation, test data\r\n","    x_data = feature[annotation != 2]\r\n","    y_data = annotation[annotation != 2]\r\n","    x_test = feature[annotation == 2]\r\n","\r\n","    # divide x/y_data into train and validation\r\n","    skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\r\n","    for train_idx, valid_idx in skf.split(x_data, y_data):\r\n","        x_train = x_data[train_idx]\r\n","        y_train = y_data[train_idx]\r\n","        x_valid = x_data[valid_idx]\r\n","        y_valid = y_data[valid_idx]\r\n","        break\r\n","\r\n","    # LightGBM parameters\r\n","    weight_column = [1./float(np.count_nonzero(y_train==0)), 1./float(np.count_nonzero(y_train==1))]\r\n","    weight_column = list(map(lambda x: x/sum(weight_column), weight_column))\r\n","    lgbm_param = {\r\n","        'objective': \"binary\",\r\n","        'learning_rate': 1e-1,\r\n","        'feature_fraction': 0.5,\r\n","        'bagging_fraction': 0.5,\r\n","        'bagging_freq': 1,\r\n","        'verbosity': -1,\r\n","        'num_leaves': my_round_int(float(param[0])/10. * (2**param[3])),\r\n","        'max_bin': param[1],\r\n","        'min_data_in_leaf': param[2],\r\n","        'max_depth': param[3],\r\n","        'weight_column': weight_column\r\n","    }\r\n","\r\n","    # training\r\n","    lgb_train = lgbm.Dataset(x_train, label=y_train)\r\n","    lgb_valid = lgbm.Dataset(x_valid, label=y_valid, reference=lgb_train)\r\n","    model = lgbm.train(lgbm_param, lgb_train, valid_sets=lgb_valid, num_boost_round=epochs, early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\r\n","\r\n","    # prediction\r\n","    y_pred_prob = model.predict(x_test, num_iteration=model.best_iteration)\r\n","\r\n","    # select query\r\n","    unannotated = np.where(annotation == 2)[0]\r\n","    tmp_arr = np.full(annotation.shape[0], np.nan, dtype=np.float64)\r\n","    tmp_arr[unannotated] = y_pred_prob\r\n","    max_uncertainty = [0., []]\r\n","\r\n","    for idx in range(0, unannotated.shape[0], query_frm):\r\n","        # calc. minimum probability\r\n","        tmp = idx+query_frm if idx+query_frm <= unannotated.shape[0] else unannotated.shape[0]\r\n","        tmp_prob = tmp_arr[unannotated[idx:tmp]]\r\n","        sum_uncertainty = sum([1.-p if p >= 0.5 else p for p in tmp_prob]) \r\n","        assert not np.isnan(sum_uncertainty)\r\n","\r\n","        # update max uncertainty\r\n","        if sum_uncertainty > max_uncertainty[0]:\r\n","            max_uncertainty = sum_uncertainty, [unannotated[idx], unannotated[tmp]]\r\n","\r\n","    y_pred = np.array([1 if p >= 0.5 else 0 for p in y_pred_prob], dtype=np.uint8)\r\n","    return y_pred, max_uncertainty[1]\r\n","\r\n","\"\"\"\r\n","Smooth predicted annotation to output elan file\r\n","\"\"\"\r\n","def postprocessing(annotation, pred_anno, y_pred, cutout):\r\n","    pred_anno[pred_anno == 2] = y_pred + 2 # manual(0,1), predicted(2,3)\r\n","    pred_anno = np.hstack((np.zeros(cutout, dtype=np.uint8), pred_anno, np.zeros(cutout, dtype=np.uint8)))\r\n","    annotation[annotation == 2] = copy.deepcopy(pred_anno[annotation == 2])\r\n","\r\n","    # separate annotation\r\n","    separated, current_anno, st = [], annotation[0], 0\r\n","    for idx in range(1, annotation.shape[0]):\r\n","        if annotation[idx] != current_anno:\r\n","            separated.append([st, idx, current_anno])\r\n","            current_anno = annotation[idx]\r\n","            st = idx\r\n","    separated.append([st, annotation.shape[0], current_anno])\r\n","    \r\n","    return annotation, separated\r\n","\r\n","def smooth_annotation(annotation, delete_frm, separated):\r\n","    if delete_frm > 0: \r\n","        # first loop (integrate consecutive short annotations)\r\n","        idx, dels = 0, []\r\n","        while True:\r\n","            if len(separated) <= idx: break\r\n","\r\n","            # short and predicted annotation\r\n","            if separated[idx][1]-separated[idx][0] <= delete_frm and separated[idx][2] > 1:\r\n","                # end\r\n","                ed = idx + 1\r\n","                while True:\r\n","                    if separated[ed][1]-separated[ed][0] > delete_frm or separated[ed][2] <= 1: break\r\n","                    else: ed += 1\r\n","\r\n","                # integration\r\n","                if idx != ed - 1:\r\n","                    separated[idx][1] = separated[ed-1][1]\r\n","                    tmp = annotation[separated[idx][0]:separated[idx][1]]\r\n","                    num_rest = np.count_nonzero(tmp == 2)\r\n","                    num_gest = np.count_nonzero(tmp == 3)\r\n","                    assert np.count_nonzero(tmp == 0) == 0 and np.count_nonzero(tmp == 1) == 0\r\n","                    label = 2 if num_rest >= num_gest else 3\r\n","                    separated[idx][2] = label\r\n","                    for idx2 in range(idx+1, ed):\r\n","                        dels.append(idx2) \r\n","                idx = ed + 1\r\n","            else: idx += 1\r\n","\r\n","        ofs = 0\r\n","        for del_ in dels:\r\n","            del separated[del_-ofs]\r\n","            ofs += 1\r\n","\r\n","        # second loop\r\n","        idx, dels = 0, []\r\n","        while True:\r\n","            if len(separated) <= idx: break\r\n","\r\n","            # short and predicted annotation\r\n","            if separated[idx][1]-separated[idx][0] <= delete_frm and separated[idx][2] > 1:\r\n","                st = idx-1 if idx != 0 else idx\r\n","                ed = idx+1 if idx < len(separated)-1 else idx\r\n","\r\n","                assert separated[st][1]-separated[st][0] > delete_frm or separated[st][2] <= 1\r\n","                assert separated[ed][1]-separated[ed][0] > delete_frm or separated[ed][2] <= 1\r\n","\r\n","                tmp = annotation[separated[st][0]:separated[ed][1]]\r\n","                num_rest = np.count_nonzero(tmp == 0) + np.count_nonzero(tmp == 2)\r\n","                num_gest = np.count_nonzero(tmp == 1) + np.count_nonzero(tmp == 3)\r\n","                label = 0 if num_rest >= num_gest else 1\r\n","                separated[idx][2] = label\r\n","            else: \r\n","                if separated[idx][2] > 1:\r\n","                    separated[idx][2] -= 2\r\n","            idx += 1\r\n","    else:\r\n","        for idx in range(len(separated)):\r\n","            if separated[idx][2] > 1:\r\n","                separated[idx][2] -= 2\r\n","\r\n","    # integration\r\n","    new_separated = []\r\n","    current_anno, st = separated[0][2], 0\r\n","    for idx, sp in enumerate(separated):\r\n","        assert sp[2] <= 1, sp[2]\r\n","        if sp[2] != current_anno:\r\n","            new_separated.append([separated[st][0], separated[idx-1][1], current_anno])\r\n","            current_anno = sp[2]\r\n","            st = idx\r\n","    new_separated.append([separated[st][0], separated[-1][1], current_anno])\r\n","    \r\n","    return new_separated\r\n","\r\n","\"\"\"\r\n","Output elan file with predicted annotation and query\r\n","\"\"\"\r\n","def output_eaf(path_elan, separated, max_uncertainty, fps):\r\n","    # Open file\r\n","    input_eaf = open(path_elan, \"r\")\r\n","    output_eaf = open(path_elan.split(\".eaf\")[0]+\"_predicted.eaf\", \"w\")\r\n","\r\n","    prev_line, examples, times = \"\", [\"\" for _ in range(6)], {}\r\n","    for line in input_eaf:\r\n","        # TIME_SLOT for manual\r\n","        if \"TIME_SLOT_ID=\\\"ts\" in line:\r\n","            split_ = line.split('\"')\r\n","            last_ts = int(split_[1].split(\"s\")[-1])\r\n","            times[split_[1]] = int(split_[3])\r\n","\r\n","        elif \"</TIME_ORDER>\" in line:\r\n","            # TIME_SLOT for predicted\r\n","            split_ = prev_line.split('\"')\r\n","            for sp_idx, sp in enumerate(separated):\r\n","                for sp_idx2 in range(2):\r\n","                    ms = int(float(sp[sp_idx2]) * 1000. / fps)\r\n","                    last_ts += 1\r\n","                    separated[sp_idx][sp_idx2] = \"ts\"+str(last_ts)\r\n","                    output_eaf.write('\"'.join([split_[0], \"ts\"+str(last_ts), split_[2], str(ms), split_[4]]))\r\n","                    if sp_idx != len(separated)-1: break\r\n","\r\n","            # TIME_SLOT for query\r\n","            for idx in range(2):\r\n","                ms = int(float(max_uncertainty[idx]) * 1000. / fps)\r\n","                last_ts += 1\r\n","                output_eaf.write('\"'.join([split_[0], \"ts\"+str(last_ts), split_[2], str(ms), split_[4]]))\r\n","            max_uncertainty = [\"ts\"+str(last_ts-1), \"ts\"+str(last_ts)]\r\n","\r\n","        # references\r\n","        elif examples[0] == \"\" and \"TIER_ID=\\\"\" in line:\r\n","            examples[0] = line.split(\"TIER_ID=\\\"\")[0]\r\n","            tab = line.split(\"<\")[0]\r\n","        elif examples[1] == \"\" and \"<ANNOTATION>\" in line:            examples[1] = line\r\n","        elif examples[2] == \"\" and \"<ALIGNABLE_ANNOTATION \" in line:  examples[2] = line.split('\"')\r\n","        elif examples[3] == \"\" and \"<ANNOTATION_VALUE>\" in line:\r\n","            examples[3] = line\r\n","            r_ann = \"rest\" if \"rest\" in line else \"gesture\"\r\n","        elif examples[4] == \"\" and \"</ALIGNABLE_ANNOTATION>\" in line: examples[4] = line\r\n","        elif examples[5] == \"\" and \"</ANNOTATION>\" in line:           examples[5] = line\r\n","        \r\n","        # manual annotation\r\n","        elif \"<ANNOTATION_VALUE>rest\" in line or \"<ANNOTATION_VALUE>gesture\" in line:\r\n","            last_a = int(prev_line.split('\"')[1].split('a')[-1])\r\n","\r\n","        # write\r\n","        elif \"</TIER>\" in prev_line and \"TIER_ID=\\\"\" not in line:\r\n","            output_eaf.write(examples[0] + \"TIER_ID=\\\"PREDICTED\\\">\\n\")\r\n","\r\n","            # write predicted annotations\r\n","            for idx, sp in enumerate(separated):\r\n","                last_a += 1\r\n","                output_eaf.write(examples[1])\r\n","                tmp = separated[idx+1][0] if idx != len(separated)-1 else sp[1]\r\n","                output_eaf.write('\"'.join([examples[2][0], \"a\"+str(last_a), examples[2][2], sp[0], examples[2][4], tmp, examples[2][6]]))\r\n","                label = \"rest\" if sp[2] == 0 else \"gesture\"\r\n","                output_eaf.write(examples[3].replace(r_ann, label))\r\n","                output_eaf.write(examples[4])\r\n","                output_eaf.write(examples[5])\r\n","            output_eaf.write(tab + \"</TIER>\\n\")\r\n","\r\n","            # write query\r\n","            output_eaf.write(examples[0] + \"TIER_ID=\\\"QUERY\\\">\\n\")\r\n","            output_eaf.write(examples[1])\r\n","            output_eaf.write('\"'.join([examples[2][0], \"a\"+str(last_a+1), examples[2][2], max_uncertainty[0], examples[2][4], max_uncertainty[1], examples[2][6]]))\r\n","            output_eaf.write(tab+tab+tab+tab + \"<ANNOTATION_VALUE>query</ANNOTATION_VALUE>\\n\")\r\n","            output_eaf.write(examples[4])\r\n","            output_eaf.write(examples[5])\r\n","            output_eaf.write(tab + \"</TIER>\\n\")\r\n","\r\n","        # update  \r\n","        output_eaf.write(line)\r\n","        prev_line = line\r\n","\r\n","    # end\r\n","    input_eaf.close()\r\n","    output_eaf.close()\r\n","    print(\"Done! {} has been outputted.\".format(path_elan.split(\".eaf\")[0]+\"_predicted.eaf\"))\r\n","\r\n","st_time = time.time()\r\n","\r\n","print(\"Making input feature array to train...\")\r\n","zipfile_ = zipfile.ZipFile(path_openpose_json_zip)\r\n","json_paths = [i for i in zipfile_.namelist() if '.json' in i]\r\n","json_paths = sorted(json_paths, key=lambda x:int(re.findall(r\"\\d+\", x)[0]))\r\n","time_window = my_round_int(float(time_window)*float(fps))\r\n","feature = make_input_feature(zipfile_, json_paths, time_window, float(th_opconf))\r\n","\r\n","print(\"Read annotation from .eaf file...\")\r\n","annotation = read_eaf(path_elan, len(json_paths), float(fps))\r\n","cutout = int((time_window-1.)/2.)\r\n","tmp_annotation = copy.deepcopy(annotation[cutout:-cutout])\r\n","\r\n","print(\"Training LGBM and predicting annotation...\")\r\n","y_pred, max_uncertainty = active_learning(feature, tmp_annotation, param, int(n_splits), int(epochs), int(early_stopping_rounds), my_round_int(float(query_sec)*float(fps)))\r\n","\r\n","print(\"Outputing elan file...\")\r\n","annotation, separated = postprocessing(annotation, tmp_annotation, y_pred, cutout)\r\n","separated = smooth_annotation(annotation, my_round_int(float(smooth_sec)*float(fps)), separated)\r\n","output_eaf(path_elan, separated, [max_uncertainty[0]+cutout, max_uncertainty[1]+cutout], float(fps))\r\n","\r\n","print(\"It took {} seconds.\".format(time.time()-st_time))"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Making input feature array to train...\n","Read annotation from .eaf file...\n","Training LGBM and predicting annotation...\n","Outputing elan file...\n","Done! /content/drive/My Drive/Colab Notebooks/me_anno_tool_predicted.eaf has been outputted.\n","It took 6.012408971786499 seconds.\n"],"name":"stdout"}]}]}